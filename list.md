## Reading List

List of reading lists and survey papers:

### Books
  
  * [Deep Learning](http://www.iro.umontreal.ca/~bengioy/dlbook/), Yoshua Bengio, Ian Goodfellow, Aaron Courville, MIT Press, In preparation.

### Review Papers

  * [Representation Learning: A Review and New Perspectives](http://arxiv.org/abs/1206.5538), Yoshua Bengio, Aaron Courville, Pascal Vincent, Arxiv, 2012.
  * The monograph or review paper: [Learning Deep Architectures for AI](http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/239), Foundations & Trends in Machine Learning, 2009.
  * [Deep Machine Learning – A New Frontier in Artificial Intelligence Research](http://www.ece.utk.edu/~itamar/Papers/DML_Arel_2010.pdf), a survey paper by Itamar Arel, Derek C. Rose, and Thomas P. Karnowski.
  * [Supervised sequence labelling with recurrent neural networks](https://www.cs.toronto.edu/~graves/preprint.pdf), Graves, A. Springer(2012).
  * [Deep Learning in Neural Networks: An Overview](https://arxiv.org/pdf/1404.7828.pdf), Schmidhuber, J. (2014). 75 pages, 850+ references.
  * [Deep learning](https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf), LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. Nature 521, no. 7553 (2015): 436-444.

### Reinforcement Learning
  
  * [Playing Atari with deep reinforcement learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf), Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. arXiv preprint arXiv:1312.5602 (2013).
  * [Recurrent Models of Visual Attention](http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf). Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu. ArXiv e-print, 2014.

### Computer Vision

  * [ImageNet Classification with Deep Convolutional Neural Networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, NIPS 2012.
  * [Going Deeper with Convolutions](https://arxiv.org/pdf/1409.4842.pdf), Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, 19-Sept-2014.
  * [Learning Hierarchical Features for Scene Labeling](http://yann.lecun.com/exdb/publis/pdf/farabet-pami-13.pdf), Clement Farabet, Camille Couprie, Laurent Najman and Yann LeCun, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013.
  * [Learning Convolutional Feature Hierachies for Visual Recognition](http://yann.lecun.com/exdb/publis/pdf/koray-nips-10.pdf), Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau, Karol Gregor, Michaël Mathieu and Yann LeCun, Advances in Neural Information Processing Systems (NIPS 2010), 23, 2010.
  * [A novel connectionist system for unconstrained handwriting recognition](http://people.idsia.ch/~juergen/tpami_2008.pdf). Graves, Alex, et al. Pattern Analysis and Machine Intelligence, IEEE Transactions on 31.5 (2009): 855-868.
  * [Deep, big, simple neural nets for handwritten digit recognition](https://arxiv.org/pdf/1003.0358.pdf). Cireşan, D. C., Meier, U., Gambardella, L. M., & Schmidhuber, J. (2010). Neural computation, 22(12), 3207-3220.
  * [Multi-column deep neural networks for image classification](https://arxiv.org/pdf/1202.2745.pdf), Ciresan, Dan, Ueli Meier, and Jürgen Schmidhuber. Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012.
  * [A committee of neural networks for traffic sign classification](http://people.idsia.ch/~masci/papers/2011_ijcnn.pdf). Ciresan, D., Meier, U., Masci, J., & Schmidhuber, J. (2011, July). In Neural Networks (IJCNN), The 2011 International Joint Conference on (pp. 1918-1921). IEEE.

### NLP and Speech

  * [Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing](https://www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php?id=en%3Apubli&cache=cache&media=en:bordes12aistats.pdf), Antoine Bordes, Xavier Glorot, Jason Weston and Yoshua Bengio (2012), in: Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS).
  * [Dynamic pooling and unfolding recursive autoencoders for paraphrase detection](https://papers.nips.cc/paper/4204-dynamic-pooling-and-unfolding-recursive-autoencoders-for-paraphrase-detection.pdf). Socher, R., Huang, E. H., Pennington, J., Ng, A. Y., and Manning, C. D. (2011a).  In NIPS’2011.
  * [Semi-supervised recursive autoencoders for predicting sentiment distributions](http://nlp.stanford.edu/pubs/SocherPenningtonHuangNgManning_EMNLP2011.pdf). Socher, R., Pennington, J., Huang, E. H., Ng, A. Y., and Manning, C. D. (2011b).  In EMNLP’2011.
  * Mikolov Tomáš: [Statistical Language Models based on Neural Networks](http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf). PhD thesis, Brno University of Technology, 2012.
  * [Framewise phoneme classification with bidirectional LSTM and other neural network architectures](). Graves, Alex, and Jürgen Schmidhuber. Neural Networks 18.5 (2005): 602-610.
  * [Distributed representations of words and phrases and their compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. In Advances in Neural Information Processing Systems, pp. 3111-3119. 2013.
  * [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf). K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, Y. Bengio. EMNLP 2014.
  * [Sequence to sequence learning with neural networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf). Sutskever, Ilya, Oriol Vinyals, and Quoc VV Le. Advances in Neural Information Processing Systems. 2014.

### Disentangling Factors and Variations with Depth

  * [Measuring invariances in deep networks](http://ai.stanford.edu/~hllee/nips09-MeasuringInvariancesDeepNetworks.pdf). Goodfellow, Ian, et al. Advances in neural information processing systems 22 (2009): 646-654.
  * [Better Mixing via Deep Representations](https://pdfs.semanticscholar.org/6bae/8b36d37d32529b956349ef4e3993fa84f062.pdf). Bengio, Yoshua, et al. arXiv preprint arXiv:1207.4404 (2012).
  * [Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach](http://www.icml-2011.org/papers/342_icmlpaper.pdf). Xavier Glorot, Antoine Bordes and Yoshua Bengio. in: Proceedings of the Twenty-eight International Conference on Machine Learning (ICML’11), pages 97-110, 2011.

### Transfer Learning and domain adaptation

  * Raina, Rajat, et al. [Self-taught learning: transfer learning from unlabeled data](http://ai.stanford.edu/~hllee/icml07-selftaughtlearning.pdf). Proceedings of the 24th international conference on Machine learning. ACM, 2007.
  * Xavier Glorot, Antoine Bordes and Yoshua Bengio, [Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach](http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/494), in: Proceedings of the Twenty-eight International Conference on Machine Learning (ICML’11), pages 97-110, 2011.
  * R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. [Natural Language Processing (Almost) from Scratch](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf). Journal of Machine Learning Research, 12:2493-2537, 2011.
  * Mesnil, Grégoire, et al. [Unsupervised and transfer learning challenge: a deep learning approach](http://www.jmlr.org/proceedings/papers/v27/mesnil12a/mesnil12a.pdf). Unsupervised and Transfer Learning Workshop, in conjunction with ICML. 2011.
  * Ciresan, D. C., Meier, U., & Schmidhuber, J. (2012, June). [Transfer learning for Latin and Chinese characters with deep neural networks](http://people.idsia.ch/~juergen/ijcnn2012transfer.pdf). In Neural Networks (IJCNN), The 2012 International Joint Conference on (pp. 1-6). IEEE.
  * Goodfellow, Ian, Aaron Courville, and Yoshua Bengio. [Large-Scale Feature Learning With Spike-and-Slab Sparse Coding](https://arxiv.org/pdf/1206.6407.pdf). ICML 2012.

### Practical Tricks and Guides
  
  * [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/pdf/1207.0580v1.pdf). Hinton, Geoffrey E., et al.  arXiv preprint arXiv:1207.0580 (2012).
  * [Practical recommendations for gradient-based training of deep architectures](https://arxiv.org/pdf/1206.5533.pdf), Yoshua Bengio, U. Montreal, arXiv report:1206.5533, Lecture Notes in Computer Science Volume 7700, Neural Networks: Tricks of the Trade Second Edition, Editors: Grégoire Montavon, Geneviève B. Orr, Klaus-Robert Müller, 2012.
  * [A practical guide to training Restricted Boltzmann Machines](http://www.cs.utoronto.ca/~hinton/absps/guideTR.pdf), by Geoffrey Hinton.

### Sparce Coding

  * [Emergence of simple-cell receptive field properties by learning a sparse code for natural images](https://courses.cs.washington.edu/courses/cse528/11sp/Olshausen-nature-paper.pdf), Bruno Olhausen, Nature 1996. cited by 4441.
  * Kavukcuoglu, Koray, Marc’Aurelio Ranzato, and Yann LeCun. [Fast inference in sparse coding algorithms with applications to object recognition](https://arxiv.org/pdf/1010.3467.pdf). arXiv preprint arXiv:1010.3467 (2010). 
  * Goodfellow, Ian, Aaron Courville, and Yoshua Bengio. [Large-Scale Feature Learning With Spike-and-Slab Sparse Coding](https://arxiv.org/pdf/1206.6407.pdf). ICML 2012.
  * [Efficient sparse coding algorithms](https://papers.nips.cc/paper/2979-efficient-sparse-coding-algorithms.pdf). Honglak Lee, Alexis Battle, Raina Rajat and Andrew Y. Ng. In NIPS 19, 2007. 
  * [Sparse coding with an overcomplete basis set: A strategy employed by VI?](http://www.chaos.gwdg.de/~michael/CNS_course_2004/papers_max/OlshausenField1997.pdf). Olshausen, Bruno A., and David J. Field. Vision research 37.23 (1997): 3311-3326.

### Foundation Theory and Motivation

  * 








