## Reading List

List of reading lists and survey papers:

### Books
  
  * [Deep Learning](http://www.iro.umontreal.ca/~bengioy/dlbook/), Yoshua Bengio, Ian Goodfellow, Aaron Courville, MIT Press, In preparation.

### Review Papers

  * [Representation Learning: A Review and New Perspectives](http://arxiv.org/abs/1206.5538), Yoshua Bengio, Aaron Courville, Pascal Vincent, Arxiv, 2012.
  * The monograph or review paper: [Learning Deep Architectures for AI](http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/239), Foundations & Trends in Machine Learning, 2009.
  * [Deep Machine Learning – A New Frontier in Artificial Intelligence Research](http://www.ece.utk.edu/~itamar/Papers/DML_Arel_2010.pdf), a survey paper by Itamar Arel, Derek C. Rose, and Thomas P. Karnowski.
  * [Supervised sequence labelling with recurrent neural networks](https://www.cs.toronto.edu/~graves/preprint.pdf), Graves, A. Springer(2012).
  * [Deep Learning in Neural Networks: An Overview](https://arxiv.org/pdf/1404.7828.pdf), Schmidhuber, J. (2014). 75 pages, 850+ references.
  * [Deep learning](https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf), LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. Nature 521, no. 7553 (2015): 436-444.

### Reinforcement Learning
  
  * [Playing Atari with deep reinforcement learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf), Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. arXiv preprint arXiv:1312.5602 (2013).
  * [Recurrent Models of Visual Attention](http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf). Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu. ArXiv e-print, 2014.

### Computer Vision

  * [ImageNet Classification with Deep Convolutional Neural Networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, NIPS 2012.
  * [Going Deeper with Convolutions](https://arxiv.org/pdf/1409.4842.pdf), Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, 19-Sept-2014.
  * [Learning Hierarchical Features for Scene Labeling](http://yann.lecun.com/exdb/publis/pdf/farabet-pami-13.pdf), Clement Farabet, Camille Couprie, Laurent Najman and Yann LeCun, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013.
  * [Learning Convolutional Feature Hierachies for Visual Recognition](http://yann.lecun.com/exdb/publis/pdf/koray-nips-10.pdf), Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau, Karol Gregor, Michaël Mathieu and Yann LeCun, Advances in Neural Information Processing Systems (NIPS 2010), 23, 2010.
  * [A novel connectionist system for unconstrained handwriting recognition](http://people.idsia.ch/~juergen/tpami_2008.pdf). Graves, Alex, et al. Pattern Analysis and Machine Intelligence, IEEE Transactions on 31.5 (2009): 855-868.
  * [Deep, big, simple neural nets for handwritten digit recognition](https://arxiv.org/pdf/1003.0358.pdf). Cireşan, D. C., Meier, U., Gambardella, L. M., & Schmidhuber, J. (2010). Neural computation, 22(12), 3207-3220.
  * [Multi-column deep neural networks for image classification](https://arxiv.org/pdf/1202.2745.pdf), Ciresan, Dan, Ueli Meier, and Jürgen Schmidhuber. Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012.
  * [A committee of neural networks for traffic sign classification](http://people.idsia.ch/~masci/papers/2011_ijcnn.pdf). Ciresan, D., Meier, U., Masci, J., & Schmidhuber, J. (2011, July). In Neural Networks (IJCNN), The 2011 International Joint Conference on (pp. 1918-1921). IEEE.

### NLP and Speech

  * [Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing](https://www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php?id=en%3Apubli&cache=cache&media=en:bordes12aistats.pdf), Antoine Bordes, Xavier Glorot, Jason Weston and Yoshua Bengio (2012), in: Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS).
  * [Dynamic pooling and unfolding recursive autoencoders for paraphrase detection](https://papers.nips.cc/paper/4204-dynamic-pooling-and-unfolding-recursive-autoencoders-for-paraphrase-detection.pdf). Socher, R., Huang, E. H., Pennington, J., Ng, A. Y., and Manning, C. D. (2011a).  In NIPS’2011.
  * [Semi-supervised recursive autoencoders for predicting sentiment distributions](http://nlp.stanford.edu/pubs/SocherPenningtonHuangNgManning_EMNLP2011.pdf). Socher, R., Pennington, J., Huang, E. H., Ng, A. Y., and Manning, C. D. (2011b).  In EMNLP’2011.
  * Mikolov Tomáš: [Statistical Language Models based on Neural Networks](http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf). PhD thesis, Brno University of Technology, 2012.
  * [Framewise phoneme classification with bidirectional LSTM and other neural network architectures](). Graves, Alex, and Jürgen Schmidhuber. Neural Networks 18.5 (2005): 602-610.
  * [Distributed representations of words and phrases and their compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. In Advances in Neural Information Processing Systems, pp. 3111-3119. 2013.
  * [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf). K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, Y. Bengio. EMNLP 2014.
  * [Sequence to sequence learning with neural networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf). Sutskever, Ilya, Oriol Vinyals, and Quoc VV Le. Advances in Neural Information Processing Systems. 2014.

### Disentangling Factors and Variations with Depth

  * [Measuring invariances in deep networks](http://ai.stanford.edu/~hllee/nips09-MeasuringInvariancesDeepNetworks.pdf). Goodfellow, Ian, et al. Advances in neural information processing systems 22 (2009): 646-654.
  * [Better Mixing via Deep Representations](https://pdfs.semanticscholar.org/6bae/8b36d37d32529b956349ef4e3993fa84f062.pdf). Bengio, Yoshua, et al. arXiv preprint arXiv:1207.4404 (2012).
  * [Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach](http://www.icml-2011.org/papers/342_icmlpaper.pdf). Xavier Glorot, Antoine Bordes and Yoshua Bengio. in: Proceedings of the Twenty-eight International Conference on Machine Learning (ICML’11), pages 97-110, 2011.

### Transfer Learning and domain adaptation

  * Raina, Rajat, et al. [Self-taught learning: transfer learning from unlabeled data](http://ai.stanford.edu/~hllee/icml07-selftaughtlearning.pdf). Proceedings of the 24th international conference on Machine learning. ACM, 2007.
  * Xavier Glorot, Antoine Bordes and Yoshua Bengio, [Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach](http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/494), in: Proceedings of the Twenty-eight International Conference on Machine Learning (ICML’11), pages 97-110, 2011.
  * R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. [Natural Language Processing (Almost) from Scratch](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf). Journal of Machine Learning Research, 12:2493-2537, 2011.
  * Mesnil, Grégoire, et al. [Unsupervised and transfer learning challenge: a deep learning approach](http://www.jmlr.org/proceedings/papers/v27/mesnil12a/mesnil12a.pdf). Unsupervised and Transfer Learning Workshop, in conjunction with ICML. 2011.
  * Ciresan, D. C., Meier, U., & Schmidhuber, J. (2012, June). [Transfer learning for Latin and Chinese characters with deep neural networks](http://people.idsia.ch/~juergen/ijcnn2012transfer.pdf). In Neural Networks (IJCNN), The 2012 International Joint Conference on (pp. 1-6). IEEE.
  * Goodfellow, Ian, Aaron Courville, and Yoshua Bengio. [Large-Scale Feature Learning With Spike-and-Slab Sparse Coding](https://arxiv.org/pdf/1206.6407.pdf). ICML 2012.

### Practical Tricks and Guides
  
  * [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/pdf/1207.0580v1.pdf). Hinton, Geoffrey E., et al.  arXiv preprint arXiv:1207.0580 (2012).
  * [Practical recommendations for gradient-based training of deep architectures](https://arxiv.org/pdf/1206.5533.pdf), Yoshua Bengio, U. Montreal, arXiv report:1206.5533, Lecture Notes in Computer Science Volume 7700, Neural Networks: Tricks of the Trade Second Edition, Editors: Grégoire Montavon, Geneviève B. Orr, Klaus-Robert Müller, 2012.
  * [A practical guide to training Restricted Boltzmann Machines](http://www.cs.utoronto.ca/~hinton/absps/guideTR.pdf), by Geoffrey Hinton.

### Sparce Coding

  * [Emergence of simple-cell receptive field properties by learning a sparse code for natural images](https://courses.cs.washington.edu/courses/cse528/11sp/Olshausen-nature-paper.pdf), Bruno Olhausen, Nature 1996. cited by 4441.
  * Kavukcuoglu, Koray, Marc’Aurelio Ranzato, and Yann LeCun. [Fast inference in sparse coding algorithms with applications to object recognition](https://arxiv.org/pdf/1010.3467.pdf). arXiv preprint arXiv:1010.3467 (2010). 
  * Goodfellow, Ian, Aaron Courville, and Yoshua Bengio. [Large-Scale Feature Learning With Spike-and-Slab Sparse Coding](https://arxiv.org/pdf/1206.6407.pdf). ICML 2012.
  * [Efficient sparse coding algorithms](https://papers.nips.cc/paper/2979-efficient-sparse-coding-algorithms.pdf). Honglak Lee, Alexis Battle, Raina Rajat and Andrew Y. Ng. In NIPS 19, 2007. 
  * [Sparse coding with an overcomplete basis set: A strategy employed by VI?](http://www.chaos.gwdg.de/~michael/CNS_course_2004/papers_max/OlshausenField1997.pdf). Olshausen, Bruno A., and David J. Field. Vision research 37.23 (1997): 3311-3326.

### Foundation Theory and Motivation

  * [Deterministic Boltzmann learning performs steepest descent in weight-space](http://www.cs.toronto.edu/~fritz/absps/dbmNC.pdf). Hinton, Geoffrey E. Neural computation 1.1 (1989): 143-150. cited by 197. 
  * [Modeling high-dimensional discrete data with multi-layer neural networks](). Bengio, Yoshua, and Samy Bengio. Advances in Neural Information Processing Systems 12 (2000): 400-406. cited by 57.
  * [Greedy layer-wise training of deep networks](https://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf). Bengio, Yoshua, et al. Advances in neural information processing systems 19 (2007): 153.
  * [Nonlocal estimation of manifold structure](http://www.iro.umontreal.ca/~lisa/pointeurs/nc-submission.pdf). Bengio, Yoshua, Martin Monperrus, and Hugo Larochelle. Neural Computation 18.10 (2006): 2509-2528.
  * [Reducing the dimensionality of data with neural networks](https://www.cs.toronto.edu/~hinton/science.pdf). Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. Science 313.5786 (2006): 504-507. cited by 4567. 
  * [Sparse feature learning for deep belief networks](https://papers.nips.cc/paper/3363-sparse-feature-learning-for-deep-belief-networks.pdf). Marc’Aurelio Ranzato, Y., Lan Boureau, and Yann LeCun. Advances in neural information processing systems 20 (2007): 1185-1192. cited by 448.
  * [Scaling learning algorithms towards AI](http://yann.lecun.com/exdb/publis/pdf/bengio-lecun-07.pdf). Bengio, Yoshua, and Yann LeCun. Large-Scale Kernel Machines 34 (2007). cited by 594.
  * [Representational power of restricted boltzmann machines and deep belief networks](http://www.iro.umontreal.ca/~lisa/publications2/index.php/attachments/single/22). Le Roux, Nicolas, and Yoshua Bengio. Neural Computation 20.6 (2008): 1631-1649.
  * [Temporal-Kernel Recurrent Neural Networks](http://www.cs.utoronto.ca/~ilya/pubs/2008/tkrnn.pdf). Sutskever, Ilya, and Geoffrey Hinton. Neural Networks 23.2 (2010): 239-243. cited by 20.
  * [Deep belief networks are compact universal approximators](https://pdfs.semanticscholar.org/a011/7ec4cd582974d06159644d12f65862a8daa3.pdf). Le Roux, Nicolas, and Yoshua Bengio. Neural computation 22.8 (2010): 2192-2207. cited by 74.
  * [On the expressive power of deep architectures](https://pdfs.semanticscholar.org/75b3/2007ae3c5dc2a4009503a3a9d6fc9614f9e7.pdf). Bengio, Yoshua, and Olivier Delalleau. Algorithmic Learning Theory. Springer Berlin/Heidelberg, 2011. cited by 110. 
  * [When Does a Mixture of Products Contain a Product of Mixtures?](http://mypages.iit.edu/~as2014/talks/morton.pdf). Montufar, Guido F., and Jason Morton. arXiv preprint arXiv:1206.0387 (2012). cited by 23. 
  * [On the Number of Linear Regions of Deep Neural Networks](https://arxiv.org/pdf/1402.1869.pdf). Montúfar, Guido, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. arXiv preprint arXiv:1402.1869 (2014). cited by 101. 


### Supervised Feedfoward Neural Networks

  * [The Manifold Tangent Classifier](https://papers.nips.cc/paper/4409-the-manifold-tangent-classifier.pdf). Salah Rifai, Yann Dauphin, Pascal Vincent, Yoshua Bengio and Xavier Muller, in: NIPS’2011. cited by 111.
  * [Discriminative Learning of Sum-Product Networks](http://homes.cs.washington.edu/~pedrod/papers/nips12.pdf). Gens, Robert, and Pedro Domingos, NIPS 2012 Best Student Paper. cited by 105.
  * [Maxout networks](https://arxiv.org/pdf/1302.4389.pdf). Goodfellow, I., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013). Technical Report, Universite de Montreal.
  * [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/pdf/1207.0580.pdf). Hinton, Geoffrey E., et al. arXiv preprint arXiv:1207.0580 (2012).
  * [Fast dropout training](http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13a.pdf). Wang, Sida, and Christopher Manning. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 118-126. 2013. cited by 103. 
  * [Deep sparse rectifier networks](http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf). Glorot, Xavier, Antoine Bordes, and Yoshua Bengio. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume, vol. 15, pp. 315-323. 2011. cited by 807. 
  * [ImageNet Classification with Deep Convolutional Neural Networks](https://www.nvidia.cn/content/tesla/pdf/machine-learning/imagenet-classification-with-deep-convolutional-nn.pdf). Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, NIPS 2012.

### Large Scale Deep Learning

  * [Building High-level Features Using Large Scale Unsupervised Learning](https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/unsupervised_icml2012.pdf). Quoc V. Le, Marc’Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeffrey Dean, and Andrew Y. Ng, ICML 2012.
  * [Neural probabilistic language models](http://www.ai.mit.edu/projects/jmlr/papers/volume3/tmp/bengio03a.pdf). Bengio, Yoshua, et al. Innovations in Machine Learning (2006): 137-186. Specifically Section 3 of this paper discusses the asynchronous SGD.
  * [Large scale distributed deep networks](http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf). Dean, Jeffrey, et al. Advances in Neural Information Processing Systems. 2012. cited by 797.

### Recurrent Networks

  * [Training Recurrent Neural Networks](http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf). Ilya Sutskever, PhD Thesis, 2012. cited by 115.
  * [Learning long-term dependencies with gradient descent is difficult](http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf). Bengio, Yoshua, Patrice Simard, and Paolo Frasconi. Neural Networks, IEEE Transactions on 5.2 (1994): 157-166.
  * Mikolov Tomáš: [Statistical Language Models based on Neural Networks](http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf). PhD thesis, Brno University of Technology, 2012.
  * [Long short-term memory](http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf). Hochreiter, Sepp, and Jürgen Schmidhuber. Neural computation 9.8 (1997): 1735-1780. cited by 3787.
  * [Gradient flow in recurrent nets: the difficulty of learning long-term dependencies](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.7321&rep=rep1&type=pdf). Hochreiter, S., Bengio, Y., Frasconi, P., & Schmidhuber, J. (2001). 
  * [Learning complex, extended sequences using the principle of history compression](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.3934&rep=rep1&type=pdf). Schmidhuber, J. (1992). Neural Computation, 4(2), 234-242. cited by 213. 
  * [Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks](http://machinelearning.wustl.edu/mlpapers/paper_files/icml2006_GravesFGS06.pdf). Graves, A., Fernández, S., Gomez, F., & Schmidhuber, J. (2006, June). In Proceedings of the 23rd international conference on Machine learning (pp. 369-376). ACM.

### Optimization

  * [Training Deep and Recurrent Neural Networks with Hessian-Free Optimization](http://www.cs.utoronto.ca/~jmartens/docs/HF_book_chapter.pdf), James Martens and Ilya Sutskever, Neural Networks: Tricks of the Trade, 2012. cited by 54.
  * [No More Pesky Learning Rates](https://arxiv.org/pdf/1206.1106.pdf). Schaul, Tom, Sixin Zhang, and Yann LeCun. arXiv preprint arXiv:1206.1106 (2012). cited by 132.
  * [Topmoumoute online natural gradient algorithm](https://papers.nips.cc/paper/3234-topmoumoute-online-natural-gradient-algorithm.pdf). Le Roux, Nicolas, Pierre-Antoine Manzagol, and Yoshua Bengio. Neural Information Processing Systems (NIPS). 2007. cited by 85.
  * [SGD-QN: Careful quasi-Newton stochastic gradient descent](http://www.jmlr.org/papers/volume10/bordes09a/bordes09a.pdf). Bordes, Antoine, Léon Bottou, and Patrick Gallinari. The Journal of Machine Learning Research 10 (2009): 1737-1754. cited by 205.
  * [Understanding the difficulty of training deep feedforward neural networks](http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf). Glorot, Xavier, and Yoshua Bengio. Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS’10). Society for Artificial Intelligence and Statistics. 2010. cited by 1112.
  * [Deep Sparse Rectifier Networks](http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf). Glorot, Xavier, Antoine Bordes, and Yoshua Bengio. Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume. Vol. 15. 2011. cited by 807.
  * [Deep learning via Hessian-free optimization](http://icml2010.haifa.il.ibm.com/papers/458.pdf). Martens, James. Proceedings of the 27th International Conference on Machine Learning (ICML). Vol. 951. 2010. cited by 337. 
  * [Flat minima](http://www.bioinf.jku.at/publications/older/3304.pdf). Hochreiter, Sepp, and Jürgen Schmidhuber. Neural Computation, 9.1 (1997): 1-42. cited by 103.
  * [Revisiting natural gradient for deep networks](https://arxiv.org/pdf/1301.3584.pdf). Pascanu, Razvan, and Yoshua Bengio. arXiv preprint arXiv:1301.3584 (2013). cited by 61. 
  * [Identifying and attacking the saddle point problem in high-dimensional non-convex optimization](https://ganguli-gang.stanford.edu/pdf/14.SaddlePoint.NIPS.pdf). Dauphin, Yann N., Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. In Advances in Neural Information Processing Systems, pp. 2933-2941. 2014. cited by 160. 

### Unsupervised Feature Learning

  * [Deep boltzmann machines](http://www.jmlr.org/proceedings/papers/v5/salakhutdinov09a/salakhutdinov09a.pdf). Salakhutdinov, Ruslan, and Geoffrey E. Hinton. Proceedings of the international conference on artificial intelligence and statistics. Vol. 5. No. 2. Cambridge, MA: MIT Press, 2009. cited by 889.
  * [Scholarpedia page](http://www.scholarpedia.org/article/Deep_belief_networks) on Deep Belief Networks.

#### Deep Boltzmann Machines
  
  * [An Efficient Learning Procedure for Deep Boltzmann Machines](http://www.utstat.toronto.edu/~rsalakhu/papers/neco_DBM.pdf), Ruslan Salakhutdinov and Geoffrey Hinton, Neural Computation August 2012, Vol. 24, No. 8: 1967 — 2006. cited by 273.
  * [Deep Boltzmann Machines and the Centering Trick](http://gregoire.montavon.name/publications/montavon-lncs12.pdf). Montavon, Grégoire, and Klaus-Robert Müller. Neural Networks: Tricks of the Trade (2012): 621-637.
  * [Efficient learning of deep boltzmann machines](http://www.jmlr.org/proceedings/papers/v9/salakhutdinov10a/salakhutdinov10a.pdf). Salakhutdinov, Ruslan, and Hugo Larochelle. International Conference on Artificial Intelligence and Statistics. 2010. cited by 271.
  * [Learning deep generative models](http://www.cs.toronto.edu/~rsalakhu/papers/Russ_thesis.pdf). Salakhutdinov, Ruslan. Diss. University of Toronto, 2009. cited by 93.
  * [Multi-prediction deep Boltzmann machines](http://papers.nips.cc/paper/5024-multi-prediction-deep-boltzmann-machines.pdf). Goodfellow, Ian, et al. Advances in Neural Information Processing Systems. 2013.

#### RBMs

  * [Unsupervised Models of Images by Spike-and-Slab RBMs](http://www.icml-2011.org/papers/591_icmlpaper.pdf), Aaron Courville, James Bergstra and Yoshua Bengio, in: ICML’2011. cited by 49. 
  * [A practical guide to training restricted Boltzmann machines](http://www.csri.utoronto.ca/~hinton/absps/guideTR.pdf). Hinton, Geoffrey. Momentum 9.1 (2010): 926. cited by 894.

### Autoencoders

  * [Regularized Auto-Encoders Estimate Local Statistics](https://pdfs.semanticscholar.org/57e1/7ce6e9a06aa8137ea355ba53073e3ffc7de6.pdf), Guillaume Alain, Yoshua Bengio and Salah Rifai, Université de Montréal, arXiv report 1211.4246, 2012. 
  * [A Generative Process for Sampling Contractive Auto-Encoders](http://icml.cc/2012/papers/910.pdf). Salah Rifai, Yoshua Bengio, Yann Dauphin and Pascal Vincent, in: ICML’2012, Edinburgh, Scotland, U.K., 2012.
  * [Contracting Auto-Encoders: Explicit invariance during feature extraction](http://www.iro.umontreal.ca/~lisa/pointeurs/ICML2011_explicit_invariance.pdf), Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot and Yoshua Bengio, in: ICML’2011.
  * [Disentangling factors of variation for facial expression recognition](https://pdfs.semanticscholar.org/f9c4/31f58565f874f76a024add2aa80717ec5cf5.pdf), Salah Rifai, Yoshua Bengio, Aaron Courville, Pascal Vincent and Mehdi Mirza, in: ECCV’2012.
  * [Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion](http://jmlr.csail.mit.edu/papers/volume11/vincent10a/vincent10a.pdf)Vincent, Pascal, et al. The Journal of Machine Learning Research 11 (2010): 3371-3408.
  * [A connection between score matching and denoising autoencoders](http://www.iro.umontreal.ca/~vincentp/Publications/smdae_techreport.pdf). Vincent, Pascal. Neural computation 23.7 (2011): 1661-1674.
  * [Marginalized denoising autoencoders for domain adaptation](https://arxiv.org/pdf/1206.4683.pdf). Chen, Minmin, et al. arXiv preprint arXiv:1206.4683 (2012).


## Miscellaneous

  * ICML
    * [ICML 2009 Workshop on Learning Feature Hierarchies](http://www.cs.toronto.edu/~rsalakhu/deeplearning/references.html).
  * [Stanford’s UFLDL Recommended Readings](http://ufldl.stanford.edu/wiki/index.php/UFLDL_Recommended_Readings).
  * [MILA - Montreal Institute for Learning Algorithms](https://mila.umontreal.ca/en/)
    * [ReadingOnDeepNetworks](http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/ReadingOnDeepNetworks)
  * [Geoffrey E. Hinton](http://www.cs.toronto.edu/~hinton/)
  * [Yoshua Bengio](http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html)
  * Memkite's [Deep Learning Bibliography](http://memkite.com/deep-learning-bibliography/).
  * Jeremy D. Jackson, PHD, [CATEGORY ARCHIVES for DEEP LEARNING](http://www.jeremydjacksonphd.com/category/deep-learning/)








